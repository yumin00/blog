---
title: "Hive에 대해 알아보자"
date: 2024-06-11T23:07:14+09:00
draft: false
categories :
- DataEngineering
---

## Hive 탄생 배경
Hive에 대해 알기 전에 먼저 탄생 배경에 대해 알아보자.

Hive는 Facebook에서 개발되었으며, Hadoop을 사용하여 대용량 데이터를 처리하기 위한 도구로 시작되었다.
Facebook은 수십 페타바이트에 달하는 데이터를 관리하고 **분석**할 수 있는 방법이 필요했기 때문에, SQL과 유사한 언어를 통해 데이터 분석을 쉽게 할 수 있도록 Hive를 만들었다.

즉, Hive는 Hadoop 내에서 대용량 데이터를 분석하기 위해 탄생했다.

## Hive
Apache Hive는 대용량 데이터 집합을 위한 데이터 웨어하우스 인프라이다. 

Hive는 HDFS에 저장되어 있는 데이터를 RDB처럼 데이터베이스, 테이블과 같은 형태로 정의하는 방법을 제공하기 때문에, SQL과 유사한 HiveQL 쿼리를 사용할 수 있도록 한다. 이를 통해, HDFS에 있는 데이터를 쿼리하고 분석할 수 있는 기능을 제공한다!

- 테이블 (Tables): Hive에서 데이터는 테이블로 조직된다. 각 테이블은 스키마(구조)를 가지고 있으며, 데이터는 HDFS에 저장된다.
- 데이터베이스 (Databases): 관련된 테이블을 논리적으로 그룹화한 것이다.
- 파티션 (Partitions): 테이블을 더 작은 단위로 나눠 효율적인 데이터 관리와 빠른 쿼리를 가능하게 한다.
- 버킷 (Buckets): 파티션된 데이터를 더 작은 부분으로 나눠 데이터 샘플링 및 효율적인 조인을 가능하게 한다.

## Hive 구성요소
![image](https://github.com/yumin00/blog/assets/130362583/bba5c937-efa1-44c2-9df9-ae9429c2a4bd)

- UI: 사용자가 쿼리 및 작업을 할 수 있는 인터페이스
- Driver: 쿼리를 입력받고, 작업을 처리한다.
- Compiler: MetaStore를 참고하여 쿼리 구문을 분석하고 실행 계획을 생성
- MetaStore: Table, DB, Partitions 의 정보 저장
- Execution Engine: 컴파일러에 의해 생성된 실행 계획을 실행

Hive의 가장 큰 특징은 MetaStore 를 관리한다는 것이다. 사용자가 HDFS에 데이터를 저장하면 Hive를 통해 데이터가 테이블 형식으로 다시 저장되는 것이 아니다! 사용자는 HDFS의 데이터를 Hive를 통해 쿼리하기 위해서는 직접 테이블 생성 및 스키마를 설정해주어야 한다. 그러면 Hive는 해당 스키마 정보를 메타스토어에 저장하는 방식이다.

## Hive 동작 방식
예를 들어, 사용자가 CSV 파일을 HDFS에 업로드했다고 가정해보자.

### 1. 파일 업로드
사용자는 HDFS에 파일을 업로드한다.

### 2. 테이블 스키마 생성
Hive를 활용하기 위해서 사용자는 해당 파일에 대한 테이블 스키마를 생성해야 한다. 이때 Hive에서 제공해주는 UI를 활용할 수 있다. (ex. Hue, Apache Zeppelin)

### 3. 쿼리문 작성
사용자는 데이터 분석을 위해 Hive가 제공하는 UI를 사용하여 쿼리문을 실행시킨다.


여기까지가 사용자가 Hive를 사용하기 위해 필요한 단계이다. 다음으로는 사용자가 쿼리문을 실행했을 때 Hive의 동작방식에 대해 알아보자.

![image](https://github.com/yumin00/blog/assets/130362583/1c1881c9-6ec4-4a43-aa18-7532e7561056)

### 1. 드라이버 단계
드라이버는 쿼리를 받아들여서 구문을 분석하고 올바른지 확인한다.   

### 2. 컴파일 단계
컴파일러는 해당 쿼리문을 논리적 실행 계획으로 변환한다. 그리고 최적화를 위하여 필터 조건을 조기에 적용하거나, 중복 연산을 제거하는 등의 작업을 진행한다.

### 3. 물리적 계획 단계
컴파일 단계에서 최적화된 논리적 실행 계획을 물리적 실행 계획으로 변환한다. 이 단계에서는 실제로 사용할 MapReduce 작업, Spark 작업 등이 생성된다.

그리고 물리적 실행 계획을 구체적인 작업으로 분해한다. 이때, 각 작업들은 서로 종속성을 가지며, 독립적으로 실행될 수 있다.

### 4. 실행 엔진 단계
- 생성된 작업을 실행 엔진(MapReduce, Spark 등)에게 제출한다
- 각 실행 엔진은 작업을 수행한 뒤에, 중간 결과를 저장하거나 다음 작업으로 전달한다.

### 5. 메타스토어 단계
컴파일러와 실행 엔진은 필요한 메타데이터를 메타스토어에서 조횐다. 쿼리 실행 중에 테이블 스키마가 변경되거나 새로운 테이블이 생성되면, 메타스토어가 업데이트 된다.

### 6. HDFS 단계
HDFS에 저장된 데이터를 읽어오고, 스캔 작업을 통해서 필요한 데이터 블록을 찾아 읽는다. 쿼리 결과로 새로운 테이블로 저장되거나 HDFS에 쓰기 작업이 필요한 경우 데이터를 HDFS에 저장한다.

### 7. 결과 반환
실행 엔진이 최종 결과를 반환하면, 드라이버는 결과를 수신하여 UI에 전달한다.

## Hadoop Ecosystem에서의 Hive
Hive는 Hadoop의 여러 구성 요소와 상호 작용한다.

- HDFS: Hive는 HDFS에 데이터를 저장하거나 조회한다.
- YARN: Hive의 쿼리는 YARN을 통해 리소스를 관리하고 스케줄링한다.
- MapReduce: Hive의 쿼리는 컴파일되어 MapReduce 작업으로 변환되고 실행된다.
- HBase: Hive는 HBase와 통합되어 NoSQL 데이터베이스에 저장된 데이터를 쿼리할 수 있다.
- Pig: Pig와 함께 사용되어 다양한 데이터 처리 작업을 수행할 수 있다.

(HBase와 Pig는 아직 공부해보지 못해서 다음에 더 자세히 알아보고자 한다.)

## 결론
### 효율적인 대규모 데이터 분석
Hive는 Hadoop에서 SQL과 같은 쿼리인 HiveQL을 제공함으로써 대규모 데이터 분석을 효율적으로 할 수 있도록 한다.

### 빠른 속도의 대규모 데이터 처리
Hive는 HDFS와 함께 작동하기 때문에 대규모 데이터를 처리할 수 있으며, MapReduce/Spark와 같은 분산 처리 엔진을 사용하여 병렬로 데이터를 처리하여 속도를 크게 향상시킨다.

### 유연한 스키마
Hive는 데이터를 저장할 때는 스키마가 필요하지 않고, 읽을 때 스키마를 사용하기 때문에 CSV/JSON/Avro 등 다양한 데이터 포맷을 지원한다.