---
title: "Hadoop에 대해 알아보자"
date: 2024-05-20T20:18:15+09:00
draft: true
categories :
- DataEngineering
---

# 1. Hadoop은 왜 생겨났을까?
하둡이 무엇인지 알아보기 전에, 먼저 하둡이 왜 탄생했는지에 대해 먼저 알아보자.

## 대용량 데이터 처리의 필요성
1990년대 후반과 200년대 초반, 기술의 반전으로 인해 데이터의 양은 급격하게 증가하기 시작했다.
점점 많아지는 데이터를 기존 데이터베이스인 RDBMS로 처리하기에는 한계가 있었다.

## 구글의 논문 발표
2003년과 2004년에 구글은 대용량 데이터 처리 문제를 해결하기 위해 두 가지 핵심 논문을 발표했다.

- 구글 파일 시스템(Google File System, GFS): 대규모 데이터를 분산된 방식으로 저장하고 관리할 수 있는 파일 시스템
- 맵리듀스(MapReduce): 대규모 데이터를 병렬로 처리할 수 있는 프로그래밍 모델

이 논문들은 대용량 데이터 처리를 효율적으로 수행할 수 있는 방법론을 제시했으며, 이는 이후에 하둡의 핵심 아이디어가 되었다.

## 하둡의 탄생
더그 커팅(Doug Cutting)과 마이크 카파렐라(Mike Cafarella)는 구글의 논문에서 영감을 받아 2006년에 하둡을 개발하기 시작했다.
더그 커팅은 원래 루씬(Lucene)이라는 오픈 소스 검색 엔진 프로젝트를 진행 중이었는데, 빅데이터 처리에 대한 필요성을 느끼고 하둡 프로젝트로 전환하게 되었다.
하둡은 초기에는 야후(Yahoo)에서 사용되었고, 이후 아파치 소프트웨어 재단의 프로젝트로 발전했다.

## 하둡의 목적 및 특징
하둡은 다음과 같은 특징을 가지고 대용량 데이터를 효율적으로 처리하고 저장하기 위해서 생겨났다.

- 대규모 데이터 처리: 기존 RDBMS가 처리할 수 없는 대용량 데이터를 효율적으로 처리
- 분산 저장: 데이터를 여러 서버에 분산 저장하여 데이터 손실을 방지
- 병렬 처리: 데이터를 병렬로 처리하여 처리 속도를 크게 향상
- 저비용: 상용 하드웨어를 활용하여 효율적인 비용으로 대규모 데이터 처리 시스템 구축

그럼 지금부터 하둡은 어떤식으로 동작하길래 위와 같은 특징을 가지고 대규모 데이터 처리를 할 수 있는지에 대해 더 자세히 알아보자!

# 2. Hadoop
하둡은 어떻게 대규모 데이터를 처리할 수 있는 것일까? 하둡은 여러 가지 요소들을 사용하여 대용량 데이터를 효율적으로 처리한다.

## 분산 파일 시스템 (HDFS - Hadoop Distributed File System)
HDFS는 데이터를 여러 서버(노드)에 분산 저장한다.

### 블록 단위 저장
HDFS는 대규모 데이터를 여러 블록으로 나누어 여러 노드에 저장한다.

> 블록
> 
> 블록이란, HDFS에서 데이터를 저장할 때 사용하는 고정 크기의 데이터 조각이다. 기본 블록 크기는 128MB이지만 설정에 따라 조정할 수 있다.

각 블록은 복제되어 저장되기 때문에 일부 노드가 고장나더라도 데이터 손실이 없다.
데이터를 분산 저장함으로써, 읽기 및 쓰기 작업을 병렬로 수행할 수 있어 처리 속도가 향상된다.

노드 사이에 데이터를 고속으로 전송한다.